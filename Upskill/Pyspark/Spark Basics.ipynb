{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a spark session with 3 threads\n",
    "spark = SparkSession.builder.appName('Misc').master(\"local[3]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+\n",
      "| name|day|month|year|\n",
      "+-----+---+-----+----+\n",
      "|Abdul| 23|    5|  81|\n",
      "| Ravi| 28|    1|2022|\n",
      "|Abdul| 23|    5|  81|\n",
      "| John| 12|   12|   6|\n",
      "| Rosy|  7|    8|  63|\n",
      "+-----+---+-----+----+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Creating a dataframe \n",
    "data_list = [(\"Ravi\",\"28\",\"1\",\"2022\"),(\"Abdul\",\"23\",\"5\",\"81\"),(\"John\",\"12\",\"12\",\"6\"),(\"Rosy\",\"7\",\"8\",\"63\"),(\"Abdul\",\"23\",\"5\",\"81\")]\n",
    "raw_df = spark.createDataFrame(data_list).toDF(\"name\",\"day\",\"month\",\"year\").repartition(3)\n",
    "raw_df.show()\n",
    "raw_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding a monotonically increasing ID\n",
    "df_1 = raw_df.withColumn(\"id\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Case statement / Creating a custom column\n",
    "df_3 = df_1.withColumn(\"year\",expr(\"\"\"\n",
    "case when year <=21 then year + 2000\n",
    "     when year <=100 then year + 1900\n",
    "     else year\n",
    "     end\n",
    "\"\"\").cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Casting using type functions\n",
    "df_4 = df_1.withColumn(\"day\",col(\"day\").cast(IntegerType())).withColumn(\"month\",col(\"month\").cast(IntegerType())).withColumn(\"year\",df_1.year.cast(IntegerType()))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop Columns\n",
    "df_5 = df_3.withColumn(\"dob\",expr(\"\"\"to_date(concat(day,'/',month,'/',year),'d/m/y')\n",
    "\"\"\"))\n",
    "df_6 = df_5.drop(\"day\",\"month\",\"year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop Duplicates\n",
    "df_7 = df_6.dropDuplicates([\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort\n",
    "df_8 = df_7.sort(expr(\"dob desc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading spark dataframe\n",
    "invoice_df = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"invoices.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|total_count|\n",
      "+-----------+\n",
      "|     541909|\n",
      "+-----------+\n",
      "\n",
      "+------------+\n",
      "|sum_quantity|\n",
      "+------------+\n",
      "|     5176450|\n",
      "+------------+\n",
      "\n",
      "+-----------------+\n",
      "|   avg_unit_price|\n",
      "+-----------------+\n",
      "|4.611113626086849|\n",
      "+-----------------+\n",
      "\n",
      "+-----------------------+\n",
      "|distinct_invoice_number|\n",
      "+-----------------------+\n",
      "|                  25900|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Agg operations\n",
    "invoice_df.select(expr(\"count(*) as total_count\")).show()\n",
    "invoice_df.select(expr(\"sum(Quantity) as sum_quantity\")).show()\n",
    "invoice_df.select(expr(\"avg(UnitPrice) as avg_unit_price\")).show()\n",
    "invoice_df.select(expr(\"count(distinct invoiceno) as distinct_invoice_number\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+-------------+\n",
      "|       country|invoiceno|sum(quantity)|\n",
      "+--------------+---------+-------------+\n",
      "|United Kingdom|   536446|          329|\n",
      "|United Kingdom|   536508|          216|\n",
      "|United Kingdom|   537018|           -3|\n",
      "|United Kingdom|   537401|          -24|\n",
      "|United Kingdom|   537811|           74|\n",
      "|United Kingdom|  C537824|           -2|\n",
      "|United Kingdom|   538895|          370|\n",
      "|United Kingdom|   540453|          341|\n",
      "|United Kingdom|   541291|          217|\n",
      "|United Kingdom|   542551|           -1|\n",
      "|United Kingdom|   542576|           -1|\n",
      "|United Kingdom|   542628|            9|\n",
      "|United Kingdom|   542886|          199|\n",
      "|United Kingdom|   542907|           75|\n",
      "|United Kingdom|   543131|          134|\n",
      "|United Kingdom|   543189|          102|\n",
      "|United Kingdom|   543265|           -4|\n",
      "|        Cyprus|   544574|          173|\n",
      "|United Kingdom|   545077|           24|\n",
      "|United Kingdom|   545300|          116|\n",
      "+--------------+---------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Group by\n",
    "invoice_df.createOrReplaceTempView(\"sales\")\n",
    "summary_sql = spark.sql(\"select country,invoiceno, sum(quantity) from sales group by 1,2\")\n",
    "summary_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "to_date() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18036\\3932966949.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Spark Write\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0minvoice_df_1\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0minvoice_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"invoicedate\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_date\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"invoicedate\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"dd-MM-yyyy H.mm\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0minvoice_df_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minvoice_df_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"week_of_year\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweekofyear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"invoicedate\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0minvoice_df_2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateOrReplaceTempView\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"sales\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m final_df = spark.sql(\"\"\"\n",
      "\u001b[1;31mTypeError\u001b[0m: to_date() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "#Spark Write\n",
    "invoice_df_1= invoice_df.withColumn(\"invoicedate\", to_date(col(\"invoicedate\"),\"dd-MM-yyyy H.mm\"))\n",
    "invoice_df_2 = invoice_df_1.withColumn(\"week_of_year\", weekofyear(col(\"invoicedate\")))\n",
    "invoice_df_2.createOrReplaceTempView(\"sales\")\n",
    "final_df = spark.sql(\"\"\"\n",
    "select country,week_of_year,\n",
    "        count(distinct invoiceno) as NumInvoices,\n",
    "        sum(quantity) as TotalQuantity,\n",
    "        round(sum(quantity*unitprice),2) as InvoiceValue from sales  \n",
    "        where year(invoicedate) = 2010\n",
    "group by 1,2\"\"\")\n",
    "final_df.coalesce(1).write.format(\"parquet\").option(\"path\",\"invoice_file.parquet\").mode(\"overwrite\").save()\n",
    "final_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+-----------+-------------+------------+------------------+\n",
      "|        country|week_of_year|NumInvoices|TotalQuantity|InvoiceValue|     running_total|\n",
      "+---------------+------------+-----------+-------------+------------+------------------+\n",
      "|      Australia|          48|          1|          107|      358.25|            358.25|\n",
      "|      Australia|          49|          1|          214|       258.9|            617.15|\n",
      "|      Australia|          50|          2|          133|      387.95|1005.0999999999999|\n",
      "|        Austria|          50|          2|            3|      257.04|            257.04|\n",
      "|        Bahrain|          51|          1|           54|      205.74|            205.74|\n",
      "|        Belgium|          48|          1|          528|       346.1|             346.1|\n",
      "|        Belgium|          50|          2|          285|      625.16|            971.26|\n",
      "|        Belgium|          51|          2|          942|      838.65|1809.9099999999999|\n",
      "|Channel Islands|          49|          1|           80|      363.53|            363.53|\n",
      "|         Cyprus|          50|          1|          917|     1590.82|           1590.82|\n",
      "+---------------+------------+-----------+-------------+------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Window Aggregations\n",
    "running_total_window = Window.partitionBy(\"Country\").orderBy(\"week_of_year\").rowsBetween(Window.unboundedPreceding,Window.currentRow)\n",
    "final_df = final_df.withColumn(\"running_total\",sum(\"InvoiceValue\").over(running_total_window))\n",
    "final_df.sort(\"Country\",\"week_of_year\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joins\n",
    "orders_list = [(\"01\", \"02\", 350, 1),\n",
    "                   (\"01\", \"04\", 580, 1),\n",
    "                   (\"01\", \"07\", 320, 2),\n",
    "                   (\"02\", \"03\", 450, 1),\n",
    "                   (\"02\", \"06\", 220, 1),\n",
    "                   (\"03\", \"01\", 195, 1),\n",
    "                   (\"04\", \"09\", 270, 3),\n",
    "                   (\"04\", \"08\", 410, 2),\n",
    "                   (\"05\", \"02\", 350, 1)]\n",
    "\n",
    "order_df = spark.createDataFrame(orders_list).toDF(\"order_id\", \"prod_id\", \"unit_price\", \"qty\")\n",
    "\n",
    "product_list = [(\"01\", \"Scroll Mouse\", 250, 20),\n",
    "                    (\"02\", \"Optical Mouse\", 350, 20),\n",
    "                    (\"03\", \"Wireless Mouse\", 450, 50),\n",
    "                    (\"04\", \"Wireless Keyboard\", 580, 50),\n",
    "                    (\"05\", \"Standard Keyboard\", 360, 10),\n",
    "                    (\"06\", \"16 GB Flash Storage\", 240, 100),\n",
    "                    (\"07\", \"32 GB Flash Storage\", 320, 50),\n",
    "                    (\"08\", \"64 GB Flash Storage\", 430, 25)]\n",
    "    \n",
    "product_df = spark.createDataFrame(product_list).toDF(\"prod_id\", \"prod_name\", \"list_price\", \"qty\")\n",
    "\n",
    "joined_df = order_df.join(product_df, order_df.prod_id == product_df.prod_id, \"left\").select(\"order_id\",\"prod_name\",\"unit_price\",order_df.qty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+----------+---+\n",
      "|order_id|          prod_name|unit_price|qty|\n",
      "+--------+-------------------+----------+---+\n",
      "|      01|32 GB Flash Storage|       320|  2|\n",
      "|      03|       Scroll Mouse|       195|  1|\n",
      "|      04|    No Product Name|       270|  3|\n",
      "|      04|64 GB Flash Storage|       410|  2|\n",
      "|      02|     Wireless Mouse|       450|  1|\n",
      "|      01|      Optical Mouse|       350|  1|\n",
      "|      05|      Optical Mouse|       350|  1|\n",
      "|      02|16 GB Flash Storage|       220|  1|\n",
      "|      01|  Wireless Keyboard|       580|  1|\n",
      "+--------+-------------------+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Fill Nulls\n",
    "joined_df = joined_df.withColumn(\"prod_name\",expr(\"coalesce(prod_name,'No Product Name')\"))\n",
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+----------+---+\n",
      "|order_id|          prod_name|unit_price|qty|\n",
      "+--------+-------------------+----------+---+\n",
      "|      01|      Optical Mouse|       350|  1|\n",
      "|      01|  Wireless Keyboard|       580|  1|\n",
      "|      01|32 GB Flash Storage|       320|  2|\n",
      "|      02|     Wireless Mouse|       450|  1|\n",
      "|      02|16 GB Flash Storage|       220|  1|\n",
      "|      03|       Scroll Mouse|       195|  1|\n",
      "|      04|               null|       270|  3|\n",
      "|      04|64 GB Flash Storage|       410|  2|\n",
      "|      05|      Optical Mouse|       350|  1|\n",
      "+--------+-------------------+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Broadcast join\n",
    "product_df = product_df.toDF(\"prod_id\", \"prod_name\", \"list_price\", \"reorder_qty\")\n",
    "product_df = product_df.withColumnRenamed(\"qty\",\"reorder_qty\")\n",
    "joined_df = order_df.join(broadcast(product_df), order_df.prod_id == product_df.prod_id, \"left\").select(\"order_id\",\"prod_name\",\"unit_price\",order_df.qty)\n",
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bucket by joins \n",
    "\n",
    "#Bucket by the join key\n",
    "spark.sql(\"create database if not exists my_db\")\n",
    "order_df.write.bucketBy(3,\"prod_id\").mode(\"Overwrite\").saveAsTable(\"my_db.orders_table_2\")\n",
    "product_df.write.bucketBy(3,\"prod_id\").mode(\"Overwrite\").saveAsTable(\"my_db.products_table_2\")\n",
    "#joined_df = orders_table_2.join(products_table_2, orders_table_1.prod_id == products_table_1.prod_id, \"left\").select(\"order_id\",\"prod_name\",\"unit_price\",order_df.qty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gong_data_env",
   "language": "python",
   "name": "gong_data_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
